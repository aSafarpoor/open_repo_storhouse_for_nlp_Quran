{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "P3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "NHzu9qskQJYS",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "# Assignment #4\n",
        "**CE4719: Deep Learing**\n",
        "\n",
        "\n",
        "*   Spring 2020\n",
        "*   http://ce.sharif.edu/courses/98-99/2/ce719-1/index.php\n",
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "**Please pay attention to these notes:**\n",
        "- the coding parts you have to complete are specified by:\n",
        "```\n",
        "    ################################################################################\n",
        "    # TODO:                                                                        #\n",
        "    ################################################################################\n",
        "    pass\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################ \n",
        "```\n",
        "- We always recommend discussion in groups for assignments. However, each student should finish all of the questions by him/herself. \n",
        "- All submitted code will be compared against all student's codes using Stanford MOSS.\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course's forum page.\n",
        "- We HIGHLY encourage you to run this notebook on Google Colab.\n",
        "- **Before starting to work on the assignment, please fill your name in the next section *AND remember to RUN the cell.***\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "cellView": "form",
        "colab_type": "code",
        "deletable": true,
        "id": "koeFly4WhvEZ",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "#@title Enter your information & \"RUN the cell!!\"\n",
        "student_id = \"\" #@param {type:\"string\"}\n",
        "student_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "0urkLzHNDJOR",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "## 3. Seq2Seq Machine Translation with Attention (41 pts + 5 Bonus pts )\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "ykQ7GalMOq1U",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "In this problem we will implement a neural network that translates sentences from French to English. This is made possible by the simple but powerful idea of the sequence to sequence network, in which two recurrent neural networks work together to transform one sequence to another. In its vanilla version, an encoder network condenses an input sequence into a vector, and a decoder network unfolds that vector into a new sequence. To improve upon this model, we’ll use an attention mechanism, which lets the decoder learn to focus over a specific range of the input sequence. <br/>\n",
        "The architecture of our network is inspired by (but not exactly similar to) the neural machine translation model proposed by [1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "VEmGY0ueXv39",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "from pprint import pprint\n",
        "import unicodedata\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "Z0FojtjueMGk",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "### 3.1 Downloading and Reading the Data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "HKhtPobmfEPc",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "The data for this problem is a set of many thousands of English to French translation pairs. First let's download the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "oE6nWGOIeLZt",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "! wget https://download.pytorch.org/tutorial/data.zip\n",
        "! unzip -q data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "F9IGe8h0ffCc",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Here is a sneak peek at our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "3tFcjYuxfPB3",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "DATA_PATH = 'data/eng-fra.txt'\n",
        "! head -n 10 $DATA_PATH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "jI_t9Pszfjqy",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "As you can see, every line contains a sentence in English and its translated version in French (Sentences are sorted by their length, so the first few sentences are very short)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "CMiPqhayYS2S",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "# Setting the device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)\n",
        "\n",
        "# Feel free to use the following variables wherever you like throughout the notebook\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "PAD_TOKEN = '<PAD>'\n",
        "\n",
        "PAD_IDX = 0\n",
        "START_IDX = 1\n",
        "END_IDX = 2\n",
        "\n",
        "\n",
        "# Function for setting the random seed for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if DEVICE == torch.device(\"cuda\"):\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "VHGg20qDMkdf",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Let's define some functions to read the data from file and preprocess it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "PNGgkIyGgSW4",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "      c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "  \n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def read_corpus(path):\n",
        "    \"\"\"Reads the corpus from the file.\n",
        "\n",
        "    path: corpus file's path\n",
        "    \"\"\"\n",
        "    print(\"Reading the corpus...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open(path, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    src_sents = [s[1].split() for s in pairs]\n",
        "    # Marking the beginning and end of target sentences with <START> and <END>\n",
        "    tgt_sents = [[START_TOKEN] + s[0].split() + [END_TOKEN] for s in pairs]\n",
        "\n",
        "    return src_sents, tgt_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "4fZBL9vn27HH",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Let's read the corpus and print a couple of sentences of it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "pfofaz073G1v",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "src_sents, tgt_sents = read_corpus(DATA_PATH)\n",
        "pprint(src_sents[:5])\n",
        "pprint(tgt_sents[:5])\n",
        "\n",
        "print('\\nNumber of sentences', len(src_sents))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "BEXjU-VyfpjZ",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "### 3.2 Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "bZ_iARJSqsqy",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "As we need the Vocab class you implemented in Problem 1 here, copy your implementaion in the following cell (or copy your code into a .py file and import it from there)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "aVKSzWaZfOku",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "s44fwNO6ftyh",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Now let's create the vocabs using our source and target sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "20WsmLSifw-a",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "src_vocab = Vocab()\n",
        "src_vocab.build(src_sents)\n",
        "tgt_vocab = Vocab()\n",
        "tgt_vocab.build(tgt_sents)\n",
        "\n",
        "# You could also set the size of the vocab and minimum frequency of the words if you'd like\n",
        "print('French Vocab Size:', src_vocab.size)\n",
        "print('English Vocab Size:', tgt_vocab.size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "xoD6TMSXXoiv",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "### 3.3 Creating Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "xiLWkpjbXtZ1",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Now Let's create datasets and dataloaders so that we can later use them for training our model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "rABkKu9JXvhA",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "class NMTData(Dataset):\n",
        "    def __init__(self, src_sents: List[List[str]], tgt_sents: List[List[str]], transform):\n",
        "        self.src_sents = src_sents\n",
        "        self.tgt_sents = tgt_sents\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sents)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        sents = {'src': self.src_sents[item], 'tgt': self.tgt_sents[item]}\n",
        "        return self.transform(sents)\n",
        "    \n",
        "\n",
        "# utility function for padding a tensor\n",
        "def pad_tensor(tensor: torch.Tensor, size: int, dim: int=-1):\n",
        "    \"\"\"\n",
        "    tensor: tensor to pad\n",
        "    size: the size to pad to\n",
        "    dim: dimension to pad\n",
        "\n",
        "    returns a new tensor padded to 'size' in dimension 'dim'\n",
        "    \"\"\"\n",
        "    pad_size = list(tensor.shape)\n",
        "    pad_size[dim] = size - tensor.size(dim)\n",
        "    pad = torch.full(pad_size, PAD_IDX, dtype=tensor.dtype, device=tensor.device)\n",
        "\n",
        "    return torch.cat([tensor, pad], dim=dim)\n",
        "\n",
        "\n",
        "# Thanks to https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418/8\n",
        "class PadCollate:\n",
        "    \"\"\"\n",
        "    a variant of callate_fn that pads according to the longest sequence in\n",
        "    a batch of sequences\n",
        "    \"\"\"\n",
        "    def __init__(self, dim=-1):\n",
        "        \"\"\"\n",
        "        dim: the dimension to be padded (e.g., dimension of time in sequences)\n",
        "        \"\"\"\n",
        "        self.dim = dim\n",
        "\n",
        "    def pad_collate(self, batch):\n",
        "        \"\"\"\n",
        "        batch: a batch of data as a list of Dictionaries\n",
        "\n",
        "        returns:\n",
        "          src_sents: tensor of source sentences\n",
        "          tgt_sents: tensor of target sentences\n",
        "          src_lengths: lengths of source sentences before padding as a list \n",
        "        \"\"\"\n",
        "\n",
        "        # sort pairs based on the lengths of source sentences\n",
        "        batch = sorted(batch, key=lambda pair: pair['src'].size(0), reverse=True)\n",
        "        src_lengths = [pair['src'].size(0) for pair in batch]\n",
        "\n",
        "        # find longest sequence\n",
        "        max_len_src = max(map(lambda pair: pair['src'].shape[self.dim], batch))\n",
        "        max_len_tgt = max(map(lambda pair: pair['tgt'].shape[self.dim], batch))\n",
        "\n",
        "        # pad according to max_len\n",
        "        batch = list(map(lambda pair: (pad_tensor(pair['src'], size=max_len_src, dim=self.dim),\n",
        "                                       pad_tensor(pair['tgt'], size=max_len_tgt, dim=self.dim)),\n",
        "                         batch))\n",
        "\n",
        "        # stack all\n",
        "        src_sents = torch.stack([pair[0] for pair in batch], dim=-1)\n",
        "        tgt_sents = torch.stack([pair[1] for pair in batch], dim=-1)\n",
        "\n",
        "        return src_sents, tgt_sents, src_lengths\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        return self.pad_collate(batch)\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"A callable transform that converts sentences to Tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab: Vocab, tgt_vocab: Vocab):\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __call__(self, sents: Dict):\n",
        "        src_sent, tgt_sent = sents['src'], sents['tgt']\n",
        "\n",
        "        return {'src': self.src_vocab.to_tensor(src_sent),\n",
        "                'tgt': self.tgt_vocab.to_tensor(tgt_sent)}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "8Y6ldOPRZ4kz",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Now let's create a sample dataloader and read a batch of data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "SuesGV3UZ-8H",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "set_seed(40719)\n",
        "dataset = NMTData(src_sents=src_sents, tgt_sents=tgt_sents, \n",
        "                  transform=ToTensor(src_vocab, tgt_vocab))\n",
        "dataloader = DataLoader(dataset, batch_size=5, shuffle=True, num_workers=0,\n",
        "                        collate_fn=PadCollate())\n",
        "\n",
        "src_sents_tensor, tgt_sents_tensor, src_lengths = next(iter(dataloader))\n",
        "print(src_sents_tensor, tgt_sents_tensor, src_lengths, sep='\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "feCVz6vd5uyL",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Now we are ready to code our Seq2Seq model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "paCGNCC25PpY",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "### 3.4 Seq2Seq with Attention Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "qsNJVJoOO8tC",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "In Machine Translation, our goal is to convert a sentence from the source language to the target language. In this assignment, we will implement a sequence-to-sequence (Seq2Seq) network with attention, to build a Neural Machine Translation (NMT) system. In this section, we describe the training procedure for the proposed NMT system:\n",
        "\n",
        "1. Given a sentence in the source language, we look up the word embeddings from the source embeddings matrix with the embedding dimension $e$.\n",
        "\n",
        "2. We feed these embeddings to the Encoder, yielding hidden states $h_{0}^{enc},...,h_{m}^{enc} \\in \\mathbb{R}^h$ where $h$ is the *hidden size* of the network and $m$ is the length of the source sequence.\n",
        "\n",
        "3. We then initialize the Decoder's first hidden state $h_0^{dec} \\in \\mathbb{R}^h$ by applying a linear projection (called *bridge*) on the Encoder's final hidden state.\n",
        "\n",
        "4. With the Decoder initialized, we must now feed it the matching sentence in the target language. On the $t$-th step, we look up the embedding for the $t$-th word (or token), $y_t \\in \\mathbb{R}^e$ where $e$ is the *embedding size*. Note that during the training procedure where we use teacher forcing, $y_t$ is the $t$-th word in the target sentence. However, in the test stage where we do not have a target sentence, the output word of the model at the previous time step will be used as $y_t$.\n",
        "\n",
        "5. We then concatenate $y_t$ with the combined-output vector from the previous timestep (the gray vector in the illustration below which is denoted by $\\tilde{h}$) to produce $\\hat{y}_{t} \\in \\mathbb{R}^{e+h}$. for the first target word (i.e. the start token), previous combined-output vector is a zero-vector. We then feed $\\hat{y}_{t}$ as input to the decoder.\n",
        "\n",
        "6. We use $h_t^{dec}$ to compute **dot attention** over $h_{0}^{enc},...,h_{m}^{enc}$ (please refer to the paper for the details about this type of attention). This will give us a combination of the Encoder's hidden states called the *context vector* (blue vector in the illustration).\n",
        "\n",
        "7. We concatenate the context vector with the decoder hidden state $h_{t}^{dec}$ at each step and apply a linear layer (called *attn_combine* in the code), Tanh, and Dropout to get the next combined-output vector.\n",
        "\n",
        "8. After doing the above procedure for all of the time steps (i.e. generating the combined outputs of all time steps), we will apply the target vocab projection layer to compute scores (logits) of producing different words in the target vocab at all of the time steps simultaneously. \n",
        "\n",
        "9. We then produce the log-probability **$P$**, using log-softmax function, over target words and sum the log probability of generating the correct word over all of the time steps. We then minimize negative value of this sum to train our network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "FlKkEZH8sidK",
        "colab_type": "text"
      },
      "source": [
        "The above algorithm will become clearer to you as we walk through their implementation step by step later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "vJ6OQ-2o6dIz",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "The following figure from the paper illustrates the attention mechanism we will implement. Note that without attention mechanism, the encoder creates a single vector which, in the ideal case, encodes the “meaning” of the input sequence into a single vector — a single point in some $h$ dimensional space of sentences. This vector will then be passed to the decoder to generate the desired output. This single vector carries the burden of encoding the entire sentence. But, with attention, the decoder network can “focus” on a different part of the encoder’s outputs for every step of the decoder’s own outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "0cS5iLuQZv-x",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"https://i.imgur.com/RUmR1CU.png\" height=400/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "PN2oUAje86jr",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Here is another illustration of the training procedure of our Seq2Seq model. Note that this figure shows stacked (multi-layer) RNNs as the Encoder and the Decoder. We will use a single layer rnn in our implementation. Note that `<s>` and `</s>` are START and END tokens [4].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "42g1NRKH9M89",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"https://imgur.com/download/Xf46eg0\" height=400/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "MhC3bIYh0i4U",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Let's start coding the model. Here is the code of our encoder's class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "AiFB2kbXz9LS",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_size, embed_size: int, src_vocab_size: int):\n",
        "        \"\"\"\n",
        "        hidden_size: hidden size to be used for the RNN\n",
        "        embed_size: embedding size (dimension)\n",
        "        src_vocab_size: source language vocab's size\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.src_vocab_size = src_vocab_size\n",
        "        self.rnn = None\n",
        "        self.src_embedding = None\n",
        "        ################################################################################\n",
        "        # TODO: IMPORTANT: INSTANTIATE OBJECTS IN THE EXACT FOLLOWING ORDER:           #\n",
        "        #       1) Instantiate self.rnn using nn.GRU.                                  #\n",
        "        #       2) Instantiate src_embedding layer which will be used for embedding    #\n",
        "        #          source sentences. (Don't forget to set the padding index to PAD_IDX)#\n",
        "        ################################################################################\n",
        "        pass\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "    def forward(self, src: torch.Tensor, src_lengths: List[int]):\n",
        "        \"\"\"\n",
        "        src: tensor of source sentences containing token's indices in the vocab\n",
        "        src_lengths: lengths of source sentences before padding\n",
        "        \"\"\"\n",
        "        enc_hiddens, last_hidden, enc_hiddens_mask = None, None, None\n",
        "        ################################################################################\n",
        "        # TODO: 1) embed the source sentences using the embedding layer.               #\n",
        "        #       2) apply the encoder rnn to the tensor of source sentences             #      \n",
        "        #          and get the encoder hidden states (or outputs) and the              #\n",
        "        #          last hidden state. Save the hidden states (outputs) and last hidden #\n",
        "        #          state in enc_hiddens and last_hidden variables. (Don't forget to    #\n",
        "        #          pack and unpack!)                                                   #\n",
        "        #       3) In its default mode, the encoder's outputs will be of the shape     #\n",
        "        #          (seq_length, batch_size, embed_size). Later when we want to apply   #\n",
        "        #          attention, we want it to be (batch_size, seq_length, embed_size).   #\n",
        "        #          So use torch.permute to reshape it into the desired shape right     #\n",
        "        #          here.                                                               #\n",
        "        #       4) generate encoder hiddens mask using generate_enc_hiddens_mask()     #\n",
        "        #          method and save the mask in enc_hiddens_mask variable.              #\n",
        "        #          this mask will be used to mask <PAD> tokens in the source sentences #\n",
        "        #          when we apply the attention mechanism in the decoder.               #\n",
        "        ################################################################################\n",
        "        pass\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "        return enc_hiddens, last_hidden, enc_hiddens_mask\n",
        "\n",
        "    def generate_enc_hiddens_mask(self, enc_hiddens: torch.Tensor, lengths: List[int]) -> torch.Tensor:\n",
        "        \"\"\"Generates mask which masks the encoder hidden states corresponding to <PAD> tokens \n",
        "        in the source sentences\n",
        "        \"\"\"\n",
        "        enc_masks = torch.zeros(enc_hiddens.shape[0], enc_hiddens.size(1), dtype=torch.float)\n",
        "        for iterator, length in enumerate(lengths):\n",
        "            enc_masks[iterator, length:] = 1\n",
        "\n",
        "        return enc_masks.to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "GSmlbil70c3p",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Here is the code of our decoder's class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "XEq2afGM0X9f",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "class AttnDecoder(nn.Module):\n",
        "    def __init__(self, hidden_size: int, embed_size: int, tgt_vocab_size: int, dropout_rate=0.2):\n",
        "        \"\"\"\n",
        "        hidden_size: hidden size to be used for the RNN\n",
        "        embed_size: embedding size (dimension)\n",
        "        tgt_vocab_size: target language vocab size \n",
        "        dropout_rate: rate to be used for the dropout layer \n",
        "        \"\"\"\n",
        "        super(AttnDecoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.tgt_vocab_size = tgt_vocab_size\n",
        "        self.rnn = None\n",
        "        self.dropout = None\n",
        "        self.tgt_vocab_projection = None  \n",
        "        self.attn_combine = None \n",
        "        self.tgt_embedding = None\n",
        "        ################################################################################\n",
        "        # TODO: IMPORTANT: INSTANTIATE OBJECTS IN THE EXACT FOLLOWING ORDER:           #\n",
        "        #       1) Instantiate the target embedding layer in self.tgt_embedding. Don't #\n",
        "        #          forget to set the padding_idx to PAD_IDX.                           #\n",
        "        #       2) Instantiate the decoder rnn (nn.GRUCell) (Try to figure out why we  #\n",
        "        #          use GRU for the encoder and GRUCell for the decoder.)               #\n",
        "        #       3) Instantiate the attn_combine layer, a Linear layer which will       #\n",
        "        #          be applied on the concatenation of context vector and decoder's     #\n",
        "        #          hidden state to get the combined output vector.                     # \n",
        "        #       4) Instantiate the tgt_vocab_projection layer which is a Linear layer  #\n",
        "        #          that will be used for projecting the combined output (denoted by    #\n",
        "        #          h_tilde in the paper) to the vocab's space.                         #\n",
        "        #       5) Instantiate the dropout layer in self.dropout                       #\n",
        "        #       IMPORTANT: SET THE BIAS OF ALL LINEAR PROJECTION LAYERS TO ZERO!       #\n",
        "        ################################################################################\n",
        "        pass\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "    def forward(self, enc_hiddens: torch.Tensor, enc_hiddens_mask: torch.Tensor, \n",
        "                dec_init_state: torch.Tensor, tgt: torch.Tensor):\n",
        "        \"\"\"Runs the teacher forcing algorithm and returns the scores (logits) of different words \n",
        "        across all of the time steps. The logits shape should be (seq_len_tgt-1, batch_size, tgt_vocab_size).\n",
        "\n",
        "        tgt: tensor of target sentences with shape (seq_len_tgt, batch_size)\n",
        "        enc_hiddens: tensor of the encoder hidden states with shape (batch_size, seq_len_src, hidden_size)\n",
        "        enc_hiddens_mask: tensor of the encoder hidden states with shape (batch_size, seq_len_src)\n",
        "        dec_init_state: decoder's initial hidden state\n",
        "        \"\"\"\n",
        "        # remove the <END> token from the longest sentence(s)\n",
        "        tgt = tgt[:-1]\n",
        "\n",
        "        # Initialize the decoder state\n",
        "        dec_state = dec_init_state\n",
        "\n",
        "        # Initialize previous combined output vector as zero\n",
        "        batch_size = enc_hiddens.size(0)\n",
        "        combined_output_prev = torch.zeros(batch_size, self.hidden_size, device=DEVICE)\n",
        "\n",
        "        # Initialize a list we will use to collect the combined output o_t at each step\n",
        "        combined_outputs = []\n",
        "        logits = None\n",
        "        ################################################################################\n",
        "        # TODO: 1) Apply the embedding layer to the target sentences tensor            #\n",
        "        #       2) Use a for loop and implement teacher forcing algorithm. At each     # \n",
        "        #          step you have to call the step method. Append the combined output of#\n",
        "        #          each step to the combined_outputs list.                             #\n",
        "        #       3) Stack the combined_outputs and make a tensor from it using          #\n",
        "        #          torch.stack                                                         #\n",
        "        #          HINT: After stacking, combined_outputs shape should be              #\n",
        "        #          (seq_len_tgt-1, batch_size, hidden_size).                           #\n",
        "        #       4) Apply the vocab projection layer to project the combined outputs to #\n",
        "        #          vocab's space and get the scores of the producing every word in the # \n",
        "        #          vocab for all of the time steps.                                    #\n",
        "        #          of the time steps. Save the scores in logits variable.              #      \n",
        "        ################################################################################\n",
        "        pass\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "        return logits\n",
        "\n",
        "    def step(self, ybar_t: torch.Tensor, dec_state: torch.Tensor, enc_hiddens: torch.Tensor, enc_hiddens_mask):\n",
        "        \"\"\"\n",
        "        ybar_t: the input to be fed to the decoder at the current step with shape\n",
        "        dec_state: decodor's previous hidden state \n",
        "        enc_hiddens: encoder hidden states (outputs)\n",
        "        enc_hiddens_mask: mask generated during the encoding phase that masks <PAD>s \n",
        "\n",
        "        returns the current hidden state and combined output\n",
        "        \"\"\"\n",
        "        attn_scores_t, dec_state_t = None, None\n",
        "        ################################################################################\n",
        "        # TODO: 1) Compute the decoder's current hidden state by applying the decoder  #\n",
        "        #          rnn on the current input and save it in dec_state_t variable.       #\n",
        "        #       2) Compute attention scores by multiplying the encoder's hidden states #\n",
        "        #          by the decoder's hidden state and save it in attn_scores_t.         #\n",
        "        #          HINT: use torch.bmm                                                 #\n",
        "        ################################################################################\n",
        "        pass\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "        # Set attn_scores_t to -inf where enc_hiddens_mask has 1, where there is a <PAD>\n",
        "        # in the source sentence.\n",
        "        if enc_hiddens_mask is not None:\n",
        "            attn_scores_t.data.masked_fill_(enc_hiddens_mask.bool(), -float('inf'))\n",
        "\n",
        "        combined_output_t = None\n",
        "        ################################################################################\n",
        "        # TODO: 1) Compute attention distribution (weights) by applying softmax        #\n",
        "        #          function on the attention scores.                                   #\n",
        "        #       2) Compute context vector by multiplying the encoder hidden states by  #\n",
        "        #          attention weights. Each hidden state should be multiplied by its    #\n",
        "        #          corresponding weight!                                               #\n",
        "        #          HINT: use torch.bmm.                                                #                \n",
        "        #       3) Compute the combined output (The gray vectors in the attention      #\n",
        "        #          illustration above) and save it in combined_output_t                #\n",
        "        #          variable.                                                           #\n",
        "        #          HINT: combined_output_t shape should be (batch_size, hidden_size).  #\n",
        "        #       4) First Apply tanh and then dropout layer on the combined_output_t.   #\n",
        "        ################################################################################\n",
        "        pass\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "        return dec_state_t, combined_output_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "Hwl7lCBVW22S",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "The following class is the whole Sequence-to-Sequence model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "ouguEZHiFhVS",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "class AttnSeq2Seq(nn.Module):\n",
        "    def __init__(self, hidden_size: int, embed_size: int, src_vocab_size: int, \n",
        "                 tgt_vocab_size: int, dropout_rate:float=0.2):\n",
        "        \"\"\"\n",
        "        hidden_size: hidden size (dimensionality) of the RNNs\n",
        "        embed_size: embedding size (dimensionality)\n",
        "        src_vocab_size: source sentences Vocabulary object\n",
        "        tgt_vocab_size: target sentences Vocabulary object\n",
        "        dropout_rate: dropout probability\n",
        "        \"\"\"\n",
        "        super(AttnSeq2Seq, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.src_vocab_size, self.tgt_vocab_size = src_vocab_size, tgt_vocab_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.encoder, self.decoder, self.bridge = None, None, None\n",
        "        ################################################################################\n",
        "        # TODO: IMPORTANT: INSTANTIATE OBJECTS IN THE EXACT FOLLOWING ORDER:           #\n",
        "        #       1) Instantiate self.encoder using Encoder class.                       #\n",
        "        #       2) Instantie the bridge between encoder and decoder. Note that bridge  #\n",
        "        #          is a Linear layer used for initializing the decoder's first hidden  #\n",
        "        #          state from encoder's last hidden state.                             #\n",
        "        #       3) Instantiate self.decoder using AttnDecoder class.                   #\n",
        "        #       IMPORTANT: SET THE BIAS OF ALL LINEAR PROJECTION LAYERS TO ZERO!       #\n",
        "        ################################################################################\n",
        "        pass\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "  \n",
        "    def init_weights(self, val: float):\n",
        "        ################################################################################\n",
        "        # TODO: Initialize all of the parameters of the model with uniform distribution#\n",
        "        #       between -val and +val.                                                 #\n",
        "        ################################################################################\n",
        "        pass\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "    def forward(self, src_tensor: torch.Tensor, tgt_tensor: torch.Tensor, src_lengths: List[int]):\n",
        "        \"\"\" Takes a mini-batch of source and target sentences, compute the log-likelihood of\n",
        "        target sentences under the language models learned by the neural machine translation\n",
        "        system.\n",
        "\n",
        "        src_tensor: tensor of source sentences \n",
        "        tgt: tensor of target sentences\n",
        "        src_lengths: lengths of source sentences before padding\n",
        "        \"\"\"\n",
        "        log_probs = None\n",
        "        ################################################################################\n",
        "        # TODO: 1) Apply the encoder to the tensor of source sentences by calling      #\n",
        "        #          self.encoder() and get the encoder hidden states (or outputs)       #\n",
        "        #          and last hidden state and the mask.                                 #\n",
        "        #       2) Compute the decoder's intial hidden state using self.bridge         #\n",
        "        #          HINT: decoder's hidden state should be a 2d tensor!                 #\n",
        "        #       3) get the outputs scores (logits) of the target words from the decoder# \n",
        "        #       4) produce the log probability over the target words using             #\n",
        "        #          F.log_softmax.                                                      #\n",
        "        ################################################################################   \n",
        "        pass\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "        tgt_mask = (tgt_tensor != PAD_IDX).float()\n",
        "\n",
        "        # Compute log probability of generating true target words\n",
        "        tgt_gold_words_log_prob = torch.gather(log_probs, index=tgt_tensor[1:].unsqueeze(-1), dim=-1).squeeze(-1) * tgt_mask[1:]\n",
        "\n",
        "        # sum scores across different time steps of sequences (i.e. computing log probability of the sequences)\n",
        "        scores = tgt_gold_words_log_prob.sum(dim=0)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def greedy_decode(self, src_sent: torch.Tensor, tgt_vocab: Vocab, max_length: int = 50):\n",
        "        \"\"\"\n",
        "        src_sent: source sentence tensor with shape (seq_len, 1)\n",
        "        max_length: allowed maximum length of the target translation\n",
        "\n",
        "        returns decoded_sent: decoded translation of source sentence\n",
        "        \"\"\"\n",
        "        decoded_sent = []\n",
        "        ################################################################################\n",
        "        # TODO: Implement the greedy decoding algorithm.                               #\n",
        "        #       First you have to run the encoder on the sourec sentence. Then run the #\n",
        "        #       decoder step by step to generate the target sentence. Note that you    #\n",
        "        #       have to directly call the step method from here.                       #\n",
        "        #       Pass the <START> token as the first input and then use the output of   #\n",
        "        #       previous step. Don't forget to concatenate combined output of previous #\n",
        "        #       time step with the input. Use all-zeros vector as the first            #\n",
        "        #       combined output.                                                       #\n",
        "        #       Stop decooding when either one of the following cases occurs:          #\n",
        "        #         1) The decoder generates an <END> token.                             #\n",
        "        #         2) The decoder generates max_length words without generating an      #\n",
        "        #            <END> token.                                                      #\n",
        "        #       Use the tgt_vocab to convert indices into words and append the         #\n",
        "        #       generated word at each time step (including <END> and exluding         #\n",
        "        #       <START>) to the decoded_sent list.                                     # \n",
        "        ################################################################################ \n",
        "        pass\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "\n",
        "        return decoded_sent\n",
        "  \n",
        "    def count_parameters(self):\n",
        "        \"\"\"Counts the number of parameters of the model.\n",
        "        \"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(model_path: str):\n",
        "        \"\"\"Loads the model from a file.\n",
        "\n",
        "        model_path: the file's path to load the model from\n",
        "        \"\"\"\n",
        "        print('loading from [%s]' % model_path)\n",
        "        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "        args = params['args']\n",
        "        model = AttnSeq2Seq(**args)\n",
        "        model.load_state_dict(params['state_dict'])\n",
        "\n",
        "    return model\n",
        "\n",
        "    def save(self, path: str):\n",
        "        \"\"\"Saves the model's parameters to a file.\n",
        "\n",
        "        path: path for the model to be saved\n",
        "        \"\"\"\n",
        "        print(\"save model's parameters to [%s]\" % path)\n",
        "\n",
        "        params = {\n",
        "            'args': dict(hidden_size=self.hidden_size,\n",
        "                         embed_size=self.embed_size,  \n",
        "                         src_vocab_size=self.src_vocab_size,\n",
        "                         tgt_vocab_size=self.tgt_vocab_size,\n",
        "                         dropout_rate=self.dropout_rate),\n",
        "            'state_dict': self.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(params, path)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "1BST0pn40Nms",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "#### 3.4.1 Initialization (6 pts)\n",
        "Implement the `__init__()` methods of the AttnSeq2Seq, Encoder, and AttnDecoder.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "Hn5-CJxw0jyj",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "#### 3.4.2 Enocder (5 pts)\n",
        "Implement `forward()` method of the Encoder class. Note that you also have to implement calling this method from the AttnSeq2Seq model too. Let's test your imeplementation of Encoder with the following simple test:\n",
        "\n",
        "**NOTE: If you do not pass the test, please double check the order of instantiations in the constructor of Encoder class!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "m7y_uIJA2a6A",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "def test_encoder():\n",
        "    set_seed(40719)\n",
        "    test_encoder = Encoder(hidden_size=3, embed_size=5, src_vocab_size=10)\n",
        "    test_encoder.to(DEVICE)\n",
        "    test_input = torch.tensor([[9, 5, 6, 8, 2, 6],\n",
        "                               [6, 5, 5, 2, 9, 0],\n",
        "                               [6, 2, 3, 0, 0, 0],\n",
        "                               [2, 3, 0, 0, 0, 0]]).t().to(DEVICE)\n",
        "\n",
        "    test_lengths = [6, 5, 3, 2]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_outputs = test_encoder(test_input, test_lengths)\n",
        "\n",
        "    enc_hiddens, last_hidden, enc_hiddens_mask = list(map(lambda x: x.cpu().numpy(), test_outputs))\n",
        "\n",
        "    assert last_hidden.shape == (1, 4, 3)\n",
        "    assert enc_hiddens.shape == (4, 6, 3)\n",
        "\n",
        "\n",
        "    assert np.linalg.norm(last_hidden - [[[-0.4525945,   0.54975474, -0.09157258],\n",
        "                                          [ 0.02271755,  0.39946806, -0.70311207],\n",
        "                                          [-0.59517485,  0.34378132, -0.02917046],\n",
        "                                          [-0.52983993,  0.38805154, -0.10685067]]]) < 1e-4\n",
        "\n",
        "\n",
        "    assert np.linalg.norm(enc_hiddens_mask - [[0., 0., 0., 0., 0., 0.],\n",
        "                                              [0., 0., 0., 0., 0., 1.],\n",
        "                                              [0., 0., 0., 1., 1., 1.],\n",
        "                                              [0., 0., 1., 1., 1., 1.]]) < 1e-8\n",
        "\n",
        "    print('passed!') \n",
        "\n",
        "\n",
        "test_encoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "1G2N4PON-qFy",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "#### 3.4.3 Decoder (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "Hcn9R61Xk235",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Implement `forward()` method of the AttnDecoder class. Note that you also have to implement calling this method from the AttnSeq2Seq model too. Let's test your imeplementation of AttnDecoder with the following simple test:\n",
        "\n",
        "**NOTE: If you do not pass the test, please double check the order of instantiations in the constructor of AttnDecoder class!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "uxKJRDjJ-sAy",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "def test_decoder():\n",
        "    set_seed(40719)\n",
        "    test_decoder = AttnDecoder(hidden_size=3, embed_size=5, tgt_vocab_size=5)\n",
        "    test_decoder.to(DEVICE)\n",
        "\n",
        "    test_enc_hiddens = torch.randn(2, 6, 3).to(DEVICE)\n",
        "    test_dec_init_state = torch.randn(2, 3).to(DEVICE)\n",
        "    test_enc_hiddens_mask = torch.tensor([[0., 0., 0., 0., 0., 0.],\n",
        "                                        [0., 0., 1., 1., 1., 1.]]).to(DEVICE)\n",
        "\n",
        "    test_tgt = torch.tensor([[1, 3, 4, 4, 2],\n",
        "                           [1, 3, 4, 2, 0]]).t().to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        logits = test_decoder(test_enc_hiddens, test_enc_hiddens_mask, \n",
        "                              test_dec_init_state, test_tgt)\n",
        "\n",
        "    assert np.linalg.norm(logits.cpu().numpy() - [[[-0.05912429,  0.24984108,  0.4239172,  -0.00661422,  0.20379552],\n",
        "                                                   [-0.06361715, -0.02126797, -0.12668079, -0.10299752, -0.03208839]],\n",
        "                                                  [[-0.03892126,  0.02662785,  0.16140112, -0.0952553,   0.13059713],\n",
        "                                                   [-0.18582398,  0.13055463,  0.08564769, -0.25669473,  0.12312718]],\n",
        "                                                  [[-0.05580753,  0.2311908,   0.2830572,   0.0229059,   0.10993659],\n",
        "                                                   [-0.1467783,   0.10387062,  0.11174537, -0.21463552,  0.1288923 ]],\n",
        "                                                  [[ 0.11575598,  0.02916381,  0.05312011,  0.22930686, -0.06500006],\n",
        "                                                   [ 0.03323233, -0.01309751,  0.01842828,  0.0455505,  -0.00356868]]]) < 1e-4\n",
        "\n",
        "    print('passed!')\n",
        "\n",
        "test_decoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "FtUhXa9XtNJL",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "#### 3.4.4 AttnSeq2Seq (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "iKyNIegcttlY",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Let's test your implementation of the `forward()` method of AttnSeq2Seq class:\n",
        "\n",
        "**NOTE: If you do not pass the test, please double check the order of instantiations in the constructors of Encoder, AttnDecoder, AttnSeq2Seq classes!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "LRTZX9EqtgwK",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "def test_attn_seq2seq():\n",
        "    set_seed(40719)\n",
        "\n",
        "    test_attn_seq2seq = AttnSeq2Seq(hidden_size=3, embed_size=5, \n",
        "                                  src_vocab_size=src_vocab.size, tgt_vocab_size=5)\n",
        "    test_attn_seq2seq.to(DEVICE)\n",
        "\n",
        "    test_src = torch.tensor([[11, 5, 6, 100, 2, 14],\n",
        "                           [2, 3, 0, 0, 0, 0]]).t().to(DEVICE)\n",
        "    test_src_lengths = [6, 2]\n",
        "    test_tgt = torch.tensor([[1, 3, 4, 4, 2],\n",
        "                          [1, 3, 4, 2, 0]]).t().to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "    scores = test_attn_seq2seq(test_src, test_tgt, test_src_lengths)\n",
        "\n",
        "    assert np.linalg.norm(scores.cpu().numpy() - [-6.272523, -4.596981]) < 1e-4\n",
        "\n",
        "    print('passed!')\n",
        "\n",
        "test_attn_seq2seq()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "PTtLqWyDqmoF",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "### 3.5 Train (9 pts + 5 Bonus pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "3z9-sONeqpGR",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Now let's implement the following function to train your network.\n",
        "\n",
        "**Note that**: \n",
        "1. you will first call the train function in the debug mode (by setting ```debug = True``` in the args) and sanity-check your network on the 100 first samples in the corpus. In this case, set the maximum number of epochs to 200. After running for this amount of epochs, you have to witness a training error very close to 0.\n",
        "\n",
        "2. take the mean of the scores returned by the network across a batch. Use the negative value of this mean as the loss of a batch. You can take a running training loss which is the mean of the losses of batches fed to the model every `log_every` iterations and print it. Note that this running loss should converges to zero during sanity check.\n",
        "\n",
        "3. When ```debug = True``` there is no need for evaluation.\n",
        "\n",
        "4. You have to report running training loss every `log_every` iteration.\n",
        "\n",
        "5. You have to evaluate your network every `val_niter` iterations (only when ```debug = False```). For evaluation, simply report the mean value of loss across all of the validation samples. DO NOT forget to set the model to eval mode!\n",
        "\n",
        "6. As well as printing train and validation losses, save them throughout training in two lists and plot them after training is finished!\n",
        "\n",
        "7. Using techniques such as gradient clipping, early stopping, and learning rate decay (use it wisely!) will have bonus points. \n",
        "\n",
        "8. Don't forget to set `dropout_rate` to zero for sanity checking.\n",
        "\n",
        "9. After doing sanity check for 200 epochs, save your model in a file (you will load it later for greedy decoding).\n",
        "\n",
        "\n",
        "After doing sanity check and making sure that your network works fine, you can train your model on the entire corpus. In that case, you have to report the loss on the validation set too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "06eUwPZ5nlE7",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "def train(args: Dict):\n",
        "    \"\"\"Trains the AttnSeq2Seq model.\n",
        "    args: arguments needed for training the model\n",
        "    \"\"\"\n",
        "    set_seed(40719)\n",
        "\n",
        "    batch_size = int(args['batch_size']) \n",
        "    log_every = int(args['log_every'])\n",
        "    debug = bool(args['debug'])\n",
        "\n",
        "    src_sents, tgt_sents = read_corpus(DATA_PATH)\n",
        "\n",
        "    if debug:\n",
        "        print('DEBUG MODE!')\n",
        "        # Use a tiny subset of sentences for sanity check\n",
        "        src_sents, tgt_sents = src_sents[:100], tgt_sents[:100]\n",
        "\n",
        "        train_dataset = NMTData(src_sents=src_sents, tgt_sents=tgt_sents, \n",
        "                                transform=ToTensor(src_vocab, tgt_vocab))\n",
        "\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
        "                                      num_workers=0, collate_fn=PadCollate())\n",
        "    else:\n",
        "        print('TRAINING ON ENTIRE CORPUS!')\n",
        "        val_ratio = 0.01\n",
        "        val_size = int(len(src_sents) * val_ratio)\n",
        "        train_size = len(src_sents) - val_size\n",
        "\n",
        "        # Splitting the dataset to validation and training set\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
        "                                      num_workers=0, collate_fn=PadCollate())\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, \n",
        "                                    num_workers=0, collate_fn=PadCollate())\n",
        "\n",
        "        val_niter = int(args['val_niter'])\n",
        "\n",
        "\n",
        "\n",
        "    model = AttnSeq2Seq(embed_size=int(args['embed_size']),\n",
        "                        hidden_size=int(args['hidden_size']),\n",
        "                        dropout_rate=float(args['dropout_rate']),\n",
        "                        src_vocab_size=src_vocab.size,\n",
        "                        tgt_vocab_size=tgt_vocab.size)\n",
        "  \n",
        "    model = model.to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=float(args['lr']))\n",
        "\n",
        "    model.init_weights(0.1)\n",
        "    model.train()\n",
        "\n",
        "    print('Begin training')\n",
        "    ################################################################################\n",
        "    # TODO: Implement the train loop.                                              #\n",
        "    ################################################################################\n",
        "    pass\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "hAN2wrtKsfaP",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Now let's call the train function and train your network on a tiny subset of the data (100 first samples) with the following cell. As we want the network to overfit on a tiny dataset, there is no need for validation set and evaluation.\n",
        "\n",
        "The hyperparameters specified for you below would probably give good results. But Feel free to change them if they do not work for you. (Do not change `debug` and `dropout_rate` hyperparams! Note that we turn off any kind of regularization during sanity check.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "xVtih06UrKmT",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "DESCRIPTION OF ARGUMENTS:\n",
        "\n",
        "log_every: determines how often (based on iterations) training loss should be printed\n",
        "out.  \n",
        "\n",
        "val_niter: hyperparameter to determine how often evaluation on validation set \n",
        "should be done (used only when you want to train on the entire corpus).\n",
        "\n",
        "max_epoch: determines maximum number of epochs you want the model to be trained.\n",
        "When you reach this number of epochs, you should break the training loop.\n",
        "\n",
        "lr: learning rate\n",
        "\n",
        "uniform_init: will indicate the low and high values used for uniform initialization \n",
        "of the weights of the model (e.g. if it is 0.1, then initial weights would be\n",
        "distributed uniformly over (-0.1, +0.1)).\n",
        "\n",
        "debug: set it to True for sanity check and to False for training on entire \n",
        "corpus.\n",
        "\n",
        "This cell should run pretty fast on GPU. Training for 200 epochs on the first 100\n",
        "samples takes roughly 10 seconds for us.\n",
        "\"\"\"\n",
        "\n",
        "args = dict(batch_size=32, embed_size=256, hidden_size=256, dropout_rate=0.0,\n",
        "            uniform_init=0.1, lr=1e-3, log_every=100, max_epoch=200, \n",
        "            val_niter=None, debug=True)\n",
        "\n",
        "train(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "t_5zo4XqyXcG",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "### 3.6 Greedy Decode and Evaluation (6 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "s3jfZbR3yRyp",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Implement `greedy_decode()` method of AttnSeq2Seq. This method will generate the translation of a single sentence in the target langauge using the greedy decoding algorithm.\n",
        "\n",
        "Test your implementation with the following function which runs greedy decoding for the model you trained and saved on the first 100 samples and prints out the sentences for which your output does not match the ground truth output. As your model has overfitted on the first 100 sentences, ALMOST all of your outputs must match the gournd truth outputs and only a couple of sentences should be printed out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "h4dRbMJKyhul",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "def evaluate_sanity():\n",
        "    ################################################################################\n",
        "    # TODO: 1) Load a pretrained model                                             #\n",
        "    ################################################################################\n",
        "    pass\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################\n",
        "    src_sents, tgt_sents = read_corpus(DATA_PATH)\n",
        "    src_sents, tgt_sents = src_sents[:100], tgt_sents[:100]\n",
        "\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    print('\\nformat of prints: [sample-index] [source-sentence] [your-translation] [ground-truth-translation]\\n')\n",
        "    for it, (src_sent, tgt_sent) in enumerate(zip(src_sents, tgt_sents)):\n",
        "        src_sent_tensor = src_vocab.to_tensor(src_sent).unsqueeze(1)\n",
        "        if ['<START>'] + model.greedy_decode(src_sent_tensor, tgt_vocab) != tgt_sent:\n",
        "            print(it+1, src_sent, ['<START>'] + model.greedy_decode(src_sent=src_sent_tensor, tgt_vocab=tgt_vocab), \n",
        "                  '\\t', tgt_sent)\n",
        "\n",
        "evaluate_sanity()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "2muncpfeynNI",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "After making sure that your network works fine, now let's set ```debug = False``` and train your network on the entire corpus. This time, you also need to report loss on the validation set. Before running the following cell, please pay attention to the notes mentioned in the text cell before the train loop. Don't forget to plot your results afterwards (feel free to return any values you need from the train function.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "zIKDn4WdcRnm",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "args = dict(batch_size=32, embed_size=256, hidden_size=256, dropout_rate=0.2,\n",
        "            uniform_init=0.1, lr=1e-3, log_every=100, max_epoch=200, \n",
        "            val_niter=1000, debug=False)\n",
        "\n",
        "train(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "otf1uVZEt9o_",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "Now let's use your trained model to generate translations of a random sample of validation sentences in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "colab_type": "code",
        "deletable": true,
        "id": "tq9JxChUuPoO",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "colab": {}
      },
      "source": [
        "def evaluate():\n",
        "    ################################################################################\n",
        "    # TODO: 1) Load a model pretrained on the entire corpus                        #\n",
        "    ################################################################################\n",
        "    pass\n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    set_seed(40719)\n",
        "    src_sents, tgt_sents = read_corpus(DATA_PATH)\n",
        "    val_ratio = 0.01\n",
        "    val_size = int(len(src_sents) * val_ratio)\n",
        "    train_size = len(src_sents) - val_size\n",
        "\n",
        "    _, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    print('\\nformat of prints: [source-sentence] [your-translation] [ground-truth-translation]\\n')\n",
        "\n",
        "    idxs = np.random.randint(0, len(val_dataset), 50)\n",
        "\n",
        "    for idx in idxs:\n",
        "        src_sent_tensor, tgt_sent_tensor = val_dataset[idx]['src'], val_dataset[idx]['tgt']\n",
        "        src_sent = [src_vocab.get_token_by_id(el.item()) for el in src_sent_tensor]\n",
        "        tgt_sent = [tgt_vocab.get_token_by_id(el.item()) for el in tgt_sent_tensor]\n",
        "        print('source sentence:\\n\\t', src_sent)\n",
        "        print('Your translation:\\n\\t', ['<START>'] + model.greedy_decode(src_sent_tensor.unsqueeze(1), tgt_vocab))\n",
        "        print('Ground truth translation:\\n\\t', tgt_sent, '\\n\\n')\n",
        "\n",
        "evaluate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "deletable": true,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "sH-PhLGEsieu",
        "colab_type": "text"
      },
      "source": [
        "**Note**: If you have implemented everything right, You should see that your network generates some of the translations correctly, some of its outputs mismatch in only a couple of words with the ground truth output, and some of them are even worse than that! <br/>\n",
        "You will witness that sometimes your network repeats generating a single word consecutively for multiple time steps and then after that the translation goes completely in the wrong way! In fact, these are all drawbacks of greedy decoding algorithm. As the network gets its own output, once one output is wrong, the network gets lost and can can generate garbage translation afterwards! There is an algorithm called **beam search** which amis to fix this issue and can generate much better outputs. But, it is outside the scope of our class. Interested students can search and read more about this algorithm on the Internet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "bPnw_vPjIiD3",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "**One final point**: Note that there are some advanced packages out there such as, spaCy and torchtext, that can help you automate many of the tasks we did above manullay (e.g. preprocessing, masking, reading the sequences, and etc.). Now that you have learned what's under the hood of training a Seq2Seq model, you can use these packages to make developing your Seq2Seq model easier from this point onwards! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG0_Hr56pJj8",
        "colab_type": "text"
      },
      "source": [
        "### 3.7 Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3r5rMBbqNpz",
        "colab_type": "text"
      },
      "source": [
        "- Check and review your answers. Make sure all cells' output are what you have planned.\n",
        "- Select File > Save.\n",
        "- To download the notebook, select File > Download .ipynb.\n",
        "- Create an archive of all notebooks (P1.ipynb, P2.ipynb, and P3.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "e392uwzbj29j",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "### 3.8 References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "colab_type": "text",
        "deletable": true,
        "id": "2FponxifRlYJ",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "source": [
        "[1] Minh-Thang Luong, Hieu Pham, Christopher D. Manning, \"Effective Approaches to Attention-based Neural Machine Translation\", CoRR 2015 https://arxiv.org/abs/1508.04025.\n",
        "\n",
        "[2] PyTorch Sequence to Sequence Machine Translation Tutorial https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n",
        "[3] CS224N, Stanford University, Assignment 4\n",
        "\n",
        "[4] https://colab.research.google.com/drive/1uFJBO1pgsiFwCGIJwZlhUzaJ2srDbtw-"
      ]
    }
  ]
}