{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IXTKYBVLYiB",
        "colab_type": "text"
      },
      "source": [
        "# Assignment #4\n",
        "**CE4719: Deep Learing**\n",
        "\n",
        "\n",
        "*   Spring 2020\n",
        "*   http://ce.sharif.edu/courses/98-99/2/ce719-1/index.php\n",
        "\n",
        "**Please pay attention to these notes:**\n",
        "\n",
        "<br/>\n",
        "\n",
        "- The items you need to answer are highlighted in red and the coding parts you need to implement are denoted by:\n",
        "```\n",
        "      ########################################\n",
        "      #     Put your implementation here     #\n",
        "      ########################################\n",
        "```\n",
        "- We always recommend discussion in groups for assignments. However, each student should finish all of the questions by him/herself. \n",
        "- All submitted code will be compared against all student's codes using Stanford MOSS.\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course's forum page.\n",
        "- We HIGHLY encourage you to run this notebook on Google Colab.\n",
        "- **Before starting to work on the assignment, please fill your name in the next section *AND remember to RUN the cell.***\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XcXOHW3sLl0",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Enter your information & \"RUN the cell!!\"\n",
        "student_id = \"\" #@param {type:\"string\"}\n",
        "student_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtknBbi1hnTE",
        "colab_type": "text"
      },
      "source": [
        "## 1. Language Model (29 + 7 pts)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NrRTj1cMHlK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Recurrent neural networks have shown excellent performance in language modeling, especially in recognizing a language's underlying structure and grammar. Having a well-trained language model can help us to understand natural language inputs, which can improve down-stream tasks. However, training a recurrent neural network has always been a challenging task. In this section, we will train a neural language model and get familiar with regularization techniques that are specially optimized for RNNs.\n",
        "\n",
        "For this section, we will use [Wikitext](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) as our dataset, which is a corpus collected from Wikipedia's featured articles. Wikitext has two distinct features: 1- It is a document-level dataset and will enable the model to learn long-term dependencies (generating a paragraph) 2- It is a well-cleaned dataset and can be used with minimum pre-processing steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsUUzKSCha5f",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxBENFBuhp66",
        "colab_type": "text"
      },
      "source": [
        "First, let's download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqkuGgaGhfBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget -q https://github.com/kazemnejad/ce4719-hw04-assets/raw/master/wikitext-2-v2.zip\n",
        "! unzip -e wikitext-2-v2.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrJ_Jueb4v3u",
        "colab_type": "text"
      },
      "source": [
        "Here is a sneak peek at our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mErcBa6q64Ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! head -n 5 wikitext-2/train.txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwEl1XIe8ULf",
        "colab_type": "text"
      },
      "source": [
        "As you can see, every line contains a full article consisting of several paragraphs (`<eol>` token is basically a `\\n`). \n",
        "\n",
        "Here is one article:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9C1PLsxAAok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(open('wikitext-2/train.txt').readlines()[13].replace(' <eol>', '\\n'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWLoaJpPAlWG",
        "colab_type": "text"
      },
      "source": [
        "In order to train the model, we need to feed the data into it. However, the shape of the input data in each iteration and how the model's input affects its performance still remain challenging. One method is to divide the dataset into its sentences and train the model on those sentences. The problem with this method is that it would never allow the model to learn inter-sentence dependencies. On the other hand, one might use paragraphs as the model's inputs. But, since our dataset is a document-level one, it would be wasteful to ignore paragraph-level relations. To sidestep these issues, we view the dataset as a single and very long string and let the model figure out all associations, such as the definition of sentences and paragraphs. In the following steps,  we will implement this method in an efficient data pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG88P6Ns1pG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "\n",
        "import random\n",
        "import pickle\n",
        "import tempfile\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "from torch.nn import LSTM\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_2oLWXk8pQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input pipeline parameters\n",
        "BATCH_SIZE = 80\n",
        "BASE_WINDOW_SIZE = 70\n",
        "VAR_BPTT_STD = 5\n",
        "VAR_BPTT_PROB = 0.95"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCzeoSDBcUBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LMDataset(IterableDataset):\n",
        "  def __init__(self, split_path: str, word2id: Dict[str, int],  \n",
        "               shuffle=False, batch_size=BATCH_SIZE, base_window_size=BASE_WINDOW_SIZE,\n",
        "               variational_bptt_window=False, variational_bptt_std=VAR_BPTT_STD,\n",
        "               variational_bptt_prob=VAR_BPTT_PROB):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "      split_path: path to the dataset file\n",
        "      shuffle: whether to shuffle the dataset\n",
        "      batch_size: the batch size\n",
        "      id2word, word2id: dataset's vocabulary\n",
        "      variational_bptt_window: \n",
        "    \"\"\"\n",
        "    super(LMDataset).__init__()\n",
        "\n",
        "    self.split_path = split_path\n",
        "    self.batch_size = batch_size\n",
        "    self.shuffle = shuffle\n",
        "    self.word2id = word2id\n",
        "    self.base_window_size = base_window_size\n",
        "    self.variational_bptt_window = variational_bptt_window\n",
        "    self.variational_bptt_std = variational_bptt_std\n",
        "    self.variational_bptt_prob = variational_bptt_prob\n",
        "\n",
        "    ### Optinal: you can put additinal codes below here ###\n",
        "    \n",
        "\n",
        "  def _initialize(self):\n",
        "    \"\"\"\n",
        "    See section 1.1.1\n",
        "\n",
        "    This method initalizes the dataset by reading \n",
        "    the file from disk and applying minimum pre-processing\n",
        "    \"\"\"\n",
        "    # A list of word ids\n",
        "    token_ids = []\n",
        "\n",
        "    ##########################################\n",
        "    #      Put your implementation here      #\n",
        "    ##########################################\n",
        "\n",
        "    self.token_ids = torch.tensor(token_ids, dtype=torch.int64)\n",
        "\n",
        "  def __next__(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    See section 1.1.2 and 1.1.3\n",
        "\n",
        "    Returns the next batch of data.\n",
        "\n",
        "    Returns:\n",
        "      x: a 2d (bs x window_size) tensor containing the input word ids\n",
        "      y: a 2d (bs x window_size) tensor, similar to x's shape, \n",
        "          containing target word ids\n",
        "\n",
        "    Dont forget to use self.batch_size as the batch size\n",
        "      and self.base_seq_len as the window size \n",
        "    \"\"\"\n",
        "    x = torch.empty([self.batch_size, self.base_window_size], \n",
        "                    dtype=torch.int64)\n",
        "    y = torch.empty([self.batch_size, self.base_window_size], \n",
        "                    dtype=torch.int64)\n",
        "    \n",
        "    ##########################################\n",
        "    #      Put your implementation here      #\n",
        "    ##########################################\n",
        "\n",
        "    return x, y\n",
        "\n",
        "  def __iter__(self):\n",
        "    \"\"\"\n",
        "    This method get called only at begining of the iteration\n",
        "    \"\"\"\n",
        "    self._initialize()\n",
        "    \n",
        "    ### Optinal: you can put additinal codes below here ###\n",
        "\n",
        "    return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1G_8mM60B9A",
        "colab_type": "text"
      },
      "source": [
        "#### 1.1.1 Read and Tokenize (3 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pNLTufp6SsH",
        "colab_type": "text"
      },
      "source": [
        "First, we need to load the dataset file and apply some preprocessing on it. Specifically, you need to: \n",
        "1. Read the file from the disk.\n",
        "2. Shuffle articles if required (please note that every line in the dataset represents a single Wikipedia article).\n",
        "3. Concatenate all articles into one big string.\n",
        "4. Tokenize it (since the dataset is already tokenized, you can get all tokens by simply splitting the big string by whitespaces).\n",
        "5. Convert all tokens into their corresponding ids (the unknown token is `<unk>`. Make sure that you will use it for out-of-vocabulary tokens)\n",
        "\n",
        "*Note: Run the following cell before implementing this section*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXeirRsz1lP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = pickle.load(open('wikitext-2/vocab.pk', 'rb'))\n",
        "word2id, id2word = vocab['word2id'], vocab['id2word']\n",
        "print(\"word2ids:\", list(word2id.items())[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxn89qBI8LuN",
        "colab_type": "text"
      },
      "source": [
        "Check your implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IGKu5zr17-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = LMDataset('wikitext-2/train.txt', shuffle=True, word2id=word2id)\n",
        "it = iter(train_ds)\n",
        "\n",
        "assert train_ds.token_ids.shape[0] == 2130493\n",
        "assert train_ds.token_ids.sum() == 3692975507\n",
        "print(\"Passed!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfOvA3wt4nVS",
        "colab_type": "text"
      },
      "source": [
        "#### 1.1.2 Create batches (6 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGMWVxo74nLQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Up to this point, we only have a single list of tokens, which basically contains the whole dataset. Training a recurrent network on such a big sequence is practically impossible since there is no way that we can backprop through millions of time-steps. Moreover, the GPU's memory is minimal, so that it will exhaust on the first few hundred steps. Therefore, we have no choice other than splitting the big string into smaller chunks. Obviously, by chunking the dataset into equal-sized sequences, the sentences or paragraphs might get cut in the middle. However, we should accept this tradeoff in order to train the model.\n",
        "\n",
        "Let's start with a straightforward chunking-technique. Imagine that we have a 40-token string; we divide it into eight 5-token substrings, and the batch size is 4:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://imgur.com/download/6HjGVpG\"/>\n",
        "</p>\n",
        "\n",
        "In each iteration, the RNN is computed independently for every row of the batch. Take the first batch as an example: The RNN for the second row starts from the token $t_6$, which can be in the middle of the original sequence. Such restriction for such a short and close dependency can significantly hurt the model's performance.\n",
        "\n",
        "Now imagine another approach to this issue. Take the original sequence of 40 elements. Divide it into 4 (the batch size) long sequences.\n",
        "<p align=\"center\">\n",
        "<img src=\"https://imgur.com/download/NXLfuaI\"/>\n",
        "</p>\n",
        "Now, given a backpropagation window size of 4, we can create the batches as following:\n",
        "<p align=\"center\">\n",
        "<img src=\"https://imgur.com/download/RFFyZHD\"/>\n",
        "</p>\n",
        "\n",
        "So let's discuss why this technique is actually better. First of all, dividing the big string by batch size (which in here is 4) produces very long rows. Each row may contain several articles (as opposed to the previous method in which each row merely contains few sentences). Therefore, most of the contents between any two consecutive rows are independent. Secondly, creating batches by moving a window across the horizontal axis enables us to use the hidden states computed from the previous batch. For instance, in the figure above, we can initialize the RNN's hidden states of the second batch with the final states of batch one. Although the gradients do not flow from the second batch to the first one, the RNN can have access to some information from the previous chunk. \n",
        "\n",
        "In this section, we are going to implement the last chunking method. Specifically, you should complete the implementation of the `LMDataset.__next__()` method. In the future, this function will be used to iterate through the dataset. Therefore, on each call, it should return the next model's inputs and ground truth labels (Note that labels are the shifted inputs in the language modeling task). To implement the method, first, divide the tokens_ids by the batch size (to have equally-sized subsequences, you should drop the remainder). Then, generate batches according to the algorithm mentioned above. Please note that the final batches may not have the same window size. Also, when there are no more batches to generate, raise a `StopIteration` exception."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4Rsgz-5zkW3",
        "colab_type": "text"
      },
      "source": [
        "Check your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd6l24KFo5Fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dummy dataset\n",
        "with tempfile.NamedTemporaryFile('w', delete=False) as f:\n",
        "  dummy_ds = ' '.join([str(i) for i in range(1, 41)]) + '\\n'\n",
        "  f.write(dummy_ds)\n",
        "  \n",
        "dummy_word2id = {str(i): i for i in range(1, 41)}\n",
        "dummy_word2id.update({'<unk>': 1})\n",
        "dummy_ds = LMDataset(f.name, shuffle=True, word2id=dummy_word2id,\n",
        "                     batch_size=4, base_window_size=5)\n",
        "xys = list(dummy_ds)\n",
        "\n",
        "assert len(xys) == 2\n",
        "assert torch.all(xys[0][0] == torch.tensor([[ 1,  2,  3,  4,  5],\n",
        "        [11, 12, 13, 14, 15],\n",
        "        [21, 22, 23, 24, 25],\n",
        "        [31, 32, 33, 34, 35]]))\n",
        "assert torch.all(xys[0][1] == torch.tensor([[ 2,  3,  4,  5,  6],\n",
        "          [12, 13, 14, 15, 16],\n",
        "          [22, 23, 24, 25, 26],\n",
        "          [32, 33, 34, 35, 36]]))\n",
        "assert torch.all(xys[1][0] == torch.tensor([[ 6,  7,  8,  9],\n",
        "          [16, 17, 18, 19],\n",
        "          [26, 27, 28, 29],\n",
        "          [36, 37, 38, 39]]))\n",
        "assert torch.all(xys[1][1] == torch.tensor([[ 7,  8,  9, 10],\n",
        "          [17, 18, 19, 20],\n",
        "          [27, 28, 29, 30],\n",
        "          [37, 38, 39, 40]]))\n",
        "\n",
        "print('Passed!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "531k4naKVEjL",
        "colab_type": "text"
      },
      "source": [
        "#### 1.1.3 Variable backpropagation window size (3.5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWePD23jYyi-",
        "colab_type": "text"
      },
      "source": [
        "Although we have managed to make the batching as efficient as possible and made use of final hidden states from the previous training iteration as an initialization for the current batch, some of the elements still receive no backpropagation window at all.  \n",
        "\n",
        "Given a window size of 5, how many dataset elements receive no backpropagation windows? (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKSf1NZQjdNQ",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPMxLbyijsRo",
        "colab_type": "text"
      },
      "source": [
        "To remedy this problem, we can use a variable window size that is randomly sampled. Such a procedure will prevent those elements from being at the same position within the backprop window among different epochs. Therefore, when given enough training epochs, we will anticipate that every element receives an adequate backpropagation window. Specifically, we use the following formula to calculate the window size for every batch:\n",
        "\n",
        "$$\n",
        "\\text{ } \\\\\n",
        "\\hat{w} = \\left\\{ \\begin{array}{rl}\n",
        "\\mathcal{W} &\\mbox{with probability} \\ p \\\\\n",
        "\\frac{1}{2} . \\mathcal{W} &\\mbox{with probability} 1-p\n",
        "\\end{array} \\right.\n",
        "\\\\ \\text{ }\n",
        "\\\\ \\text{ }\n",
        "w \\sim \\mathcal{N}(\\hat{w}, \\sigma)\n",
        "$$\n",
        "\n",
        "where $\\mathcal{W}$ is the default window size, $p$ is the a probability close to 1, $\\sigma$ is the standard deviation, and $w$ is the final window size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QmBf4eJjsIu",
        "colab_type": "text"
      },
      "source": [
        "In this section, you should edit the `__next__(self)` method to incoporate a variable window size. Since we will the same class for our validation data, the variable mechanism should be only activated when `self.variational_bptt_window` flag is true.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-XvoqNcvPJh",
        "colab_type": "text"
      },
      "source": [
        "Check your implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdk4CVgCrAZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xys = list(LMDataset('wikitext-2/train.txt', shuffle=True, word2id=word2id, \n",
        "                    variational_bptt_window=True))[:100]\n",
        "assert np.mean([x.shape[1] for x, y in xys]) != BASE_WINDOW_SIZE\n",
        "\n",
        "xys = list(LMDataset('wikitext-2/train.txt', shuffle=True, word2id=word2id, \n",
        "                    variational_bptt_window=False))[:100]\n",
        "assert np.mean([x.shape[1] for x, y in xys]) == BASE_WINDOW_SIZE\n",
        "print('Passed!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kobosW-_yUkQ",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Implementing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDC8IwgJStsd",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"https://imgur.com/download/LeK6lQh\"/>\n",
        "</p>\n",
        "\n",
        "For this assignment, we will use a simple stacked LSTM architecture and various regularized techniques to improve the model's performance and overfitting. Our primary regularization method is Dropout. But, vanilla Dropout is not quite suitable for RNNs. So, as we will explain in the following sections, some modifications are required. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-PZvzAzYQcP",
        "colab_type": "text"
      },
      "source": [
        "#### 1.2.1 ConnectionDrop LSTM (Optional: 6 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8jADAEnYY7e",
        "colab_type": "text"
      },
      "source": [
        "The problem with the standard Dropout in the context of RNNs is that it generates a new binary mask every time it gets called. Howerver, such behavior is problematic in RNNs. If we use the original Dropout on the hidden-to-hidden connection, then the RNN cell will receive a completely different mask for every timestep (in a non-recurrent layer, the Dropout mask for its inputs remains consistent through forward and backward flow). One of the methods to solve that issue is applying Dropout on the layer/cell's weight (instead of activation), which is similar to removing connections between two consecutive layers. As this Dropout only gets called once per each training step, the repeated computation of an RNN cell would not be a problem.\n",
        "\n",
        "In this section, we would like to extend the vanilla LSTM layer so that it applies weight dropout on its hidden-to-hidden connection. The corresponding weight for the mentioned connection is called `weight_hh_l0`. Implement this functionality in the below class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBZ7tlBTjQZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConnectionDropLSTM(LSTM):\n",
        "  def __init__(self, *args, dropout_rate=0.0, **kwargs):\n",
        "    super(ConnectionDropLSTM, self).__init__(*args, **kwargs)\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.is_renamed = False\n",
        "\n",
        "  def flatten_parameters(self, *arg, **kwargs):\n",
        "    # Do not change this. This is a temp fix for a wierd \n",
        "    # issue with cudnn LSTM\n",
        "    return\n",
        "\n",
        "  def _rename_orig_param(self):\n",
        "    # 1- Register parameter `weight_hh_l0` as a new parameter called `weight_hh_l0_orig`\n",
        "    # 2- Remove `weight_hh_l0` from the model's parameters\n",
        "    # hint: see the documentation for torch.nn.Module.register_parameter and\n",
        "    #          torch.nn.Module._parameters\n",
        "\n",
        "    ##########################################\n",
        "    #      Put your implementation here      #\n",
        "    ##########################################\n",
        "\n",
        "    self.is_renamed = True\n",
        "\n",
        "  def _drop_connections(self):\n",
        "    \"\"\"\n",
        "    Applies dropout on hidden-to-hidden paramters\n",
        "    \"\"\"\n",
        "\n",
        "    # 1- Create a new tensor by applying dropout on `weight_hh_l0_orig`\n",
        "    # 2- Assing the newly created tensor to instance attribute `weight_hh_l0`\n",
        "\n",
        "    ##########################################\n",
        "    #      Put your implementation here      #\n",
        "    ##########################################\n",
        "\n",
        "  def forward(self, *args, **kwargs):\n",
        "    if not self.is_renamed: \n",
        "      self._rename_orig_param()\n",
        "    \n",
        "    self._drop_connections()\n",
        "    return super().forward(*args, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IHQpouFyE1h",
        "colab_type": "text"
      },
      "source": [
        "Check your implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BObXrExoxox6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cell = ConnectionDropLSTM(20, 20, dropout_rate=0.1)\n",
        "\n",
        "x = torch.ones(3, 1, 20)\n",
        "out1, _ = cell(x)\n",
        "\n",
        "cell_params = list(cell.named_parameters())\n",
        "\n",
        "assert len(cell_params) == 4\n",
        "assert 'weight_hh_l0_orig' in [n for n,_ in cell_params]\n",
        "assert 'weight_hh_l0' not in [n for n,_ in cell_params]\n",
        "assert isinstance(cell.weight_hh_l0_orig, torch.nn.Parameter)\n",
        "assert hasattr(cell, 'weight_hh_l0')\n",
        "assert not isinstance(cell.weight_hh_l0, torch.nn.Parameter) and \\\n",
        "       cell.weight_hh_l0.requires_grad\n",
        "\n",
        "out2, _ = cell(x)\n",
        "\n",
        "assert torch.all(out2[0] == out1[0])\n",
        "assert torch.all(out2[1] != out1[1])\n",
        "assert torch.all(out2[2] != out1[2])\n",
        "\n",
        "print('Passed!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPCwAR9-D-rT",
        "colab_type": "text"
      },
      "source": [
        "#### 1.2.2 Locked Dropout (1.5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrRMKx-PJR-x",
        "colab_type": "text"
      },
      "source": [
        "Other than hidden-to-hidden connections, an RNN cell has two another  connections: input and output. Fortunately, we will use a much simpler solution to sidestep the aforementioned problem. Locked Dropout is another version of Dropout that generates the mask only once per iterations and then applies the mask to all corresponding connections within the timesteps. For more information, see figure 2 and pay attention to symbols on each \"Locked Dropout\" block. Similar symbols mean that they use the same mask on their corresponding connections. Implement this method in the following function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm3WFXoWJS7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def locked_dropout(activations: torch.Tensor, dropout_rate=0.1, training=True):\n",
        "  \"\"\"\n",
        "  Applies the same binary mask across the sequence length axis\n",
        "\n",
        "  Args:\n",
        "    activations (torch.Tensor(shape=[batch, seq_len, x])): A tensor that\n",
        "        we applies the dropout on\n",
        "    drouput_rate (float): Dropout rate (0 = no dropout, 1 = zero out all units)\n",
        "    training (bool): whether we are at the training phase\n",
        "\n",
        "  Returns:\n",
        "    torch.Tensor(shape=[batch, seq_len, x]): activations after applying the mask\n",
        "\n",
        "  Hint: Do not use PyTorch's built-in dropout function. You should implement it \n",
        "      yourself. Be careful about the differences between training and inference\n",
        "      phases.\n",
        "  \"\"\"\n",
        "  # 1- Create a unique binary mask for each element within the batch\n",
        "  # 2- Repead the masks across the sequence length\n",
        "  # 3- Compute the scalar product between the mask and activations\n",
        "\n",
        "  mask = torch.ones_like(activations, dtype=torch.float)\n",
        "\n",
        "  ##########################################\n",
        "  #      Put your implementation here      #\n",
        "  ##########################################\n",
        "\n",
        "  return mask * activations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_7_pNBANuLr",
        "colab_type": "text"
      },
      "source": [
        "Check you implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79W_coPhNtwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.ones((100, 3, 4), dtype=torch.float)\n",
        "x_drop = locked_dropout(x, dropout_rate=0.4)\n",
        "\n",
        "assert x.shape == x_drop.shape\n",
        "assert torch.abs(x_drop.sum() - x.sum()) < 100\n",
        "assert torch.all(x_drop[:, 0, :] == x_drop[:, 1, :])\n",
        "x_drop2 = locked_dropout(x, dropout_rate=0.4)\n",
        "assert torch.any(x_drop != x_drop2)\n",
        "print('Passed!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI5Xujfqe_H6",
        "colab_type": "text"
      },
      "source": [
        "#### 1.2.3 Embedding + Masking (1.5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9owYA3xn1Ke",
        "colab_type": "text"
      },
      "source": [
        "One of the applications of Dropout in standard Neural Networks is putting noise on the input vector. Applying Dropout on a single d-dim input vector might zero out some of its dimensions. But, using the exact same Dropout technique for the RNNs input, i.e., sentences, barely masks some units in some words' vector, while the base unit in RNNs input is a word. Moreover, several occurrences of the same word may receive different masking. To mitigate the problem, we mask out a word from embedding entirely. That is, if we would like to drop a word in the input layer of the network, we will change the whole row corresponding to that word in the embedding matrix to zero. See more info in [1]\n",
        "\n",
        "In the following function, you should apply this technique to the weight matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WINOK5PuoCZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mask_embedding(embed_weight: torch.Tensor, dropout_rate=0.1, training=True):\n",
        "  \"\"\"\n",
        "  Applies Dropout at word-level\n",
        "\n",
        "  Args:\n",
        "    embed_weight (torch.Tensor(shape=[vocab_size, embed_dim]): the embedding matrix\n",
        "    dropout_rate (float): dropout rate\n",
        "    training (bool): whether we are at the training phase\n",
        "\n",
        "  Returns:\n",
        "    torch.Tensor(shape=[vocab_size, embed_dim]): Masked embeddingg matrix\n",
        "\n",
        "  Hint: Do not use PyTorch built-in dropout function. You should implement it \n",
        "      yourself. Be careful about the differences between training and inference\n",
        "      phases.\n",
        "  \"\"\"\n",
        "\n",
        "  ##########################################\n",
        "  #      Put your implementation here      #\n",
        "  ##########################################\n",
        "\n",
        "  return embed_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c32Wbu_aqgmL",
        "colab_type": "text"
      },
      "source": [
        "Check your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx22jgtyqjhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_weight = torch.ones((10, 4), dtype=torch.float)\n",
        "masked_weight = mask_embedding(embed_weight, dropout_rate=0.3)\n",
        "\n",
        "assert masked_weight.shape == embed_weight.shape\n",
        "\n",
        "print(\"non masked:\")\n",
        "print(embed_weight)\n",
        "print(\"masked:\")\n",
        "print(masked_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq9_cs03sitZ",
        "colab_type": "text"
      },
      "source": [
        "#### 1.2.4 Weight tying (1.5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK-reRwL0PYR",
        "colab_type": "text"
      },
      "source": [
        "Weight tying ties the embedding matrix to the final softmax layer's weights (vocab projection), i.e., these two layers will be forced to share the same set of parameters. Such sharing reduces the number of model parameters and prevents the model from learning a one-to-one relation between the input weights and the output weights. For a theoretical justification of this work, please refer to [2].\n",
        "\n",
        "*Note that you will implement this in section 1.2.6*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6Qd-pk50Qed",
        "colab_type": "text"
      },
      "source": [
        "#### 1.2.5 Activity Regularization and Temporal Activation Regularization (3 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YfFjma-03_S",
        "colab_type": "text"
      },
      "source": [
        "**Activity Regularization** penalizes the model on having a hidden state with very large values, effectively making it have more stable hidden states. Here is the formulation:\n",
        "$$\n",
        "\\alpha L_2(m \\circ h_t)\n",
        "$$\n",
        "where $L_2(x) = \\Vert x \\Vert$, and $m \\circ h_t$ is the dropped hidden state for the last layer (AR only applies to the last RNN layer).\n",
        "\n",
        "**Temporal Activation Regularization** penalizes the model on having a large or sudden change between two consecutive hidden states, which helps the model to have more consistent hidden states. Here is the formulation:\n",
        "$$\n",
        "\\beta \\ . L_2(h_t - h_{t-1})\n",
        "$$\n",
        "Same as the AR, TAR only applies to the last RNN layer.\n",
        "Find more info in [3]\n",
        "\n",
        "*Note that you will implement this in section 1.2.6*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeSDDzLB6KdO",
        "colab_type": "text"
      },
      "source": [
        "#### 1.2.6 The model (3 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUaLAI5O6PHF",
        "colab_type": "text"
      },
      "source": [
        "After implementing different components of our extended LSTM language model, now it's time to put them all together. Use the functions and classes that you have implemented in the previous sections to fill the model's class below. Your model should have the following features:\n",
        "1. ConnectionDrop LSTM (Optional): Use this class instead of vanilla LSTM\n",
        "2. Use the `locked_dropout` at appropriate places in the model (You may want to refer to figure 2)\n",
        "3. Apply the masked dropout on the embedding matrix.\n",
        "4. Tie the weight of the embedding layer and the final softmax layer (Note that you should tie the non-masked version of the embedding matrix).\n",
        "5. You should also implement the AR and TAR, as mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj2bL6PtdPJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Hyper parameters\n",
        "NUM_LSTM_LAYER = 3\n",
        "HIDDEN_SIZE = 1150\n",
        "EMBED_DIM = 400\n",
        "DR_EMBED = 0.1\n",
        "DR_INPUT = 0.65\n",
        "DR_HIDDEN = 0.2\n",
        "DR_OUTPUT = 0.4\n",
        "CONN_DR = 0.5\n",
        "AR_COEF = 2\n",
        "TAR_COEF = 1\n",
        "VOCAB_SIZE = len(word2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S__RgI1Y9Gmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExtendedLM(nn.Module):\n",
        "  def __init__(self, num_lstm_layers=NUM_LSTM_LAYER, hidden_size=HIDDEN_SIZE, \n",
        "               vocab_size=VOCAB_SIZE, embed_dim=EMBED_DIM, \n",
        "               drop_rate_embed=DR_EMBED, drop_rate_input=DR_INPUT, \n",
        "               drop_rate_hidden=DR_HIDDEN, drop_rate_output=DR_OUTPUT, \n",
        "               lstm_conndrop_rate=CONN_DR, ar_coeff=AR_COEF, tar_coeff=TAR_COEF):\n",
        "    super(ExtendedLM, self).__init__()\n",
        "\n",
        "    ##########################################\n",
        "    #      Put your implementation here      #\n",
        "    ##########################################\n",
        "\n",
        "  def init_zero_state(self, batch_size):\n",
        "    \"\"\"\n",
        "    Returns the initial state (h_0, c_0) for LSTM networks.\n",
        "    Both h_0 and c_0 are created by non-trainable \n",
        "    tensors containing zeros\n",
        "\n",
        "    Arg:\n",
        "      batch_size (int): batch size for the input data\n",
        "\n",
        "    Returns:\n",
        "      List[Tuple(\n",
        "        torch.Tensor(shape=[1, batch_size, hidden_size]),\n",
        "        torch.Tensor(shape=[1, batch_size, hidden_size])\n",
        "      )]\n",
        "\n",
        "      it returns one tuple per each LSTM layer\n",
        "    \"\"\"\n",
        "    ##########################################\n",
        "    #      Put your implementation here      #\n",
        "    ##########################################\n",
        "\n",
        "    return []\n",
        "\n",
        "  def forward(self, inputs, initial_hiddens):\n",
        "    \"\"\"\n",
        "    Forward pass of the model\n",
        "\n",
        "    Args:\n",
        "      inputs (torch.Tensor(shape=[batch_size, seq_len], dtype=torch.int)):\n",
        "          Input of the model, which is a 2D tensor containing input word ids\n",
        "      initial_hidden (List[Tuple(\n",
        "                        torch.Tensor(shape=[1, batch_size, hidden_size]),\n",
        "                        torch.Tensor(shape=[1, batch_size, hidden_size])\n",
        "                      )]): the initial state for the LSTM modules. On the first call\n",
        "                            they are all zero. but, all other next batches are just \n",
        "                            previous state computed during the previous batch\n",
        "    Returns:\n",
        "      predictions (torch.Tensor(batch_size, seq_len, vocab_size)): the output of the model\n",
        "      final_hidden (List[Tuple(\n",
        "                        torch.Tensor(shape=[1, batch_size, hidden_size]),\n",
        "                        torch.Tensor(shape=[1, batch_size, hidden_size])\n",
        "                      )]): computed hidden state from the last step of LSTMs\n",
        "      regularization_losses torch.Tensor(shape=[], dtype=float): Accumulated for \n",
        "          regularization methods. \n",
        "    \"\"\"\n",
        "    ##########################################\n",
        "    #      Put your implementation here      #\n",
        "    ##########################################\n",
        "\n",
        "    raise NotImplemented()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv0NYG-jQm6B",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Training (6 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMgG6KVCQu0W",
        "colab_type": "text"
      },
      "source": [
        "Now it is time to implement the train-and-eval loop for our model. You should use the LMDataset class that you have implemented in section 1.1 . In addition, please pay attention to these notes:\n",
        "\n",
        "1. Remember to detatch the previous hiddens states from the computational graph in every training itereration. Otherwise, the model will backpropagate all through the begining of dataset\n",
        "2. Use gradient clipping\n",
        "3. Don't forget to use the GPU. Enable it using `model = model.cuda()`\n",
        "4. Remember to add the regularization terms to the final loss\n",
        "5. Use `Adam` as your optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5cGtnbic1iC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training hyperparameters\n",
        "NUM_EPOCH = 200\n",
        "SEED = 1543\n",
        "EVAL_INTERVAL = 500\n",
        "GRAD_CLIP = 0.25\n",
        "LEARNING_RATE = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpTuVNu_RbSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, ds):\n",
        "  \"\"\"\n",
        "  Evaluate the model on the validation set, and return loss and perplexity\n",
        "\n",
        "  Args:\n",
        "    model (ExtendedLM): An instance of the model\n",
        "    ds (LMDataset): The target dataset to calculate the performance\n",
        "\n",
        "  Returns:\n",
        "    loss (float): Average loss\n",
        "    ppl (float): Average perplexity\n",
        "\n",
        "  Hint: Do not forget to put the model in the evaluation mode\n",
        "  \"\"\"\n",
        "  ##########################################\n",
        "  #      Put your implementation here      #\n",
        "  ##########################################\n",
        "\n",
        "\n",
        "def train(model_path='lm_model.pt'):\n",
        "  \"\"\"\n",
        "  Train and save the model on the disk\n",
        "\n",
        "  Args:\n",
        "    model_path (str): Where to save checkpoints\n",
        "\n",
        "  Returns:\n",
        "    loss (float): The training loss\n",
        "    ppl (float): The training perplexity\n",
        "    model (ExtendedLM): The trained model\n",
        "  \"\"\"\n",
        "  # Set the seed to have a reproducable experiments\n",
        "  torch.manual_seed(SEED)\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  np.random.seed(SEED)\n",
        "\n",
        "  # Load the training & validation dataset\n",
        "  train_ds = LMDataset(\n",
        "      'wikitext-2/train.txt', word2id=word2id, shuffle=True, variational_bptt_window=True)\n",
        "  valid_ds = LMDataset(\n",
        "      'wikitext-2/valid.txt', word2id=word2id, batch_size=10)\n",
        "  \n",
        "  ##########################################\n",
        "  #      Put your implementation here      #\n",
        "  ##########################################\n",
        "\n",
        "  raise NotImplemented()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0fA5GfgEW5S",
        "colab_type": "text"
      },
      "source": [
        "Mount Google Drive (Optional):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys58M6ZfEV6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "gdrive_path = Path('/gdrive')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(str(gdrive_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lH9_eAI3MbU",
        "colab_type": "text"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqRRjAmA_6Lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'ce4718_hw04_lm.pt'\n",
        "if 'gdrive_path' in dir():\n",
        "  model_path = str(gdrive_path / 'My Drive' / model_name)\n",
        "else:\n",
        "  model_path = model_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt6THtBqGE0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, ppl, model = train(model_path)\n",
        "print(\"train_loss =\", loss, \"train_ppl =\", ppl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByNWh4kmLUMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clear the GPU memory\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yjSXSxZSVBo",
        "colab_type": "text"
      },
      "source": [
        "Measure the model's performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTbdO6OVSYcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_ds = LMDataset('wikitext-8/valid.txt', word2id=word2id)\n",
        "val_loss, val_ppl = evaluate(model, valid_ds)\n",
        "print(\"valid_loss =\", val_loss, \"valid_ppl =\", val_ppl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmtZqJ4lQqfy",
        "colab_type": "text"
      },
      "source": [
        "### 1.4 Text Generation (Optional: 1 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ-fGqs_XvlO",
        "colab_type": "text"
      },
      "source": [
        "Now, as the model has been trained on the dataset, it would be a good time to have some fun with it. In this section, we are going to use our language model to generate text. Specifically, given a small span of text, we want our model to complete the rest of it. To implement this, we simply input the initial text to the model, and then we predict the next words step-by-step. Predicting the next word can be achieved by taking the word that has the highest probability in the model's outputs (i.e., using `argmax`). However, it has been shown that naively taking the most probable word is not the best way to do it. Instead, we should sample the next word from the distribution the model puts on the vocabulary for each step (i.e., sampling from the model output).\n",
        "\n",
        "Implement this mechanism in the following function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Muyr8TkXuga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate(input_words, model,  word2id, id2word, max_length, temperature):\n",
        "  \"\"\"\n",
        "  Generate text given the input words\n",
        "\n",
        "  Args:\n",
        "    input_words (str): the input words. It needs tokenization\n",
        "    model (ExtendedLM): the model\n",
        "    word2id (Dict[str, int]), id2word (List[str]): the vocab\n",
        "    max_length (int): the maximum number of words to generate\n",
        "    temperature (float): the softmax temperature. A value between 0 and 1.\n",
        "          see https://en.wikipedia.org/wiki/Softmax_function for more information.\n",
        "\n",
        "  Returns:\n",
        "    List[str]: generated text\n",
        "  \"\"\"\n",
        "  ##########################################\n",
        "  #      Put your implementation here      #\n",
        "  ##########################################\n",
        "\n",
        "  raise NotImplemented()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2am-drncrP7v",
        "colab_type": "text"
      },
      "source": [
        "Check your implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsQtHocnB1kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate(\"A complete edition including the three downloadable content packs was released a year \", model, word2id, id2word, 10, 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT-v6nOprSJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate(### Your input ###\n",
        "         model, word2id, id2word, 10, 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG0_Hr56pJj8",
        "colab_type": "text"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3r5rMBbqNpz",
        "colab_type": "text"
      },
      "source": [
        "- Check and review your answers. Make sure all cells' output are what you have planned.\n",
        "- Select File > Save.\n",
        "- To download the notebook, select File > Download .ipynb.\n",
        "- Create an archive of all notebooks (P1.ipynb, P2.ipynb, and P3.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccYOoVeEMg2V",
        "colab_type": "text"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjE3oxWjjcf1",
        "colab_type": "text"
      },
      "source": [
        "1.   Gal, Yarin, and Zoubin Ghahramani. A Theoretically Grounded Application of Dropout in Recurrent Neural Networks. ArXiv:1512.05287 [Stat], October 5, 2016. http://arxiv.org/abs/1512.05287.\n",
        "2.  Inan, Hakan, Khashayar Khosravi, and Richard Socher. TYING WORD VECTORS AND WORD CLASSIFIERS: A LOSS FRAMEWORK FOR LANGUAGE MODELING. In ICLR 2017, 13, 2017.\n",
        "3. Merity, Stephen, Bryan McCann, and Richard Socher. Revisiting Activation Regularization for Language RNNs. ArXiv:1708.01009 [Cs], August 3, 2017. http://arxiv.org/abs/1708.01009.\n",
        "4. Merity, Stephen, Nitish Shirish Keskar, and Richard Socher. Regularizing and Optimizing LSTM Language Models. ArXiv:1708.02182 [Cs], August 7, 2017. http://arxiv.org/abs/1708.02182.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}